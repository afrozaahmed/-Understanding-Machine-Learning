Let's take a closer look at the process of creating a model, of training a model. We start with our training data, which we have worked with until it's beautiful, pristine, just what we need. Because we're using supervised learning, the target value is part of the training data. In the case of the credit card example, for instance, that target value is whether a transaction is fraudulent or not. Our first problem is to choose the features that we think will be most predictive of that target value. For example, in the credit card case, maybe we decide that the country in which the card was issued, the country it's used in, and the age of the user are the most likely features to help us predict whether it's fraudulent. We've chosen, let's say, features 1, 3, and 6 in our training data. We then input that training data into our chosen learning algorithm. But notice this. We only send in 75% say of all the data for the features we've chosen. Why is that? I'll tell you in a minute. But first, think about this. How do we decide which features were most predictive, and how do we choose a learning algorithm? There are lots of options as we've seen. Well the answer is if it's a simple problem or maybe our technology is simple for machine learning, the choices can be limited, not too hard. If we have a more complex problem though with lots of data and a powerful machine learning technology with lots of algorithms, this can be hard. If we have, for example, training data that has, I don't know, how about 100 features, how about 200, which ones are predictive? How many should we use, 5, 10, 50? The answer is this is what data scientists are for. This is why people who have knowledge and facility with these technologies, as well as domain knowledge about some particular problem are so valuable. It's because they can help us to do this. It can be a hard problem. In any case, the result of this is to generate a candidate model. The next problem is to work out whether or not this model is any good, and so we do that in supervised learning like this. We input test data to the candidate model. That test data is the remaining 25%, the data we held back for the features we're using, in this case 1, 3, and 6. We use that data because our candidate model can now generate target values from that test data. But here's the thing. We know what those target values should be because they are in the training data. All we have to do is compare the target values produced by a candidate model from the test data with the real target values, which are in the training data. That's how we can figure out whether or not our model is predictive or not when we're doing supervised learning. Suppose it's not. Suppose our model is just not very good. How can we improve it? Well, there's some mutual options. One of them is maybe we've chosen the wrong features. Let's choose different ones. How about 1, 2, and 5 this time? Or maybe it's the case that we have the wrong data. Let's get some new data or at least some more example data. Or maybe the problem is the algorithm. Maybe it's the case that we can modify some parameters in our algorithm, they commonly have them, or choose another one entirely. Whatever we do will generate another candidate model, and we'll test it, and the process repeats. It iterates. Now iteration is a fancy way of saying trial and error. So don't be confused. This process is called machine learning, but notice how much people do. People make decisions about features, about algorithms, about parameters. The process is very human, even though it's called machine learning.